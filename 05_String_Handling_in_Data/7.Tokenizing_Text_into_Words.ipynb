{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742b7601",
   "metadata": {},
   "source": [
    "# Topic 05 - Problem 07: Tokenizing Text into Words\n",
    "\n",
    "---\n",
    "\n",
    "## 1. About the Problem\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units, typically words or phrases.  \n",
    "This is the first step in most Natural Language Processing (NLP) tasks.\n",
    "\n",
    "In this problem, I will **tokenize** a product description into **individual words**.  \n",
    "This process is fundamental for:\n",
    "- Text analysis\n",
    "- Sentiment analysis\n",
    "- Text classification\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77bc97",
   "metadata": {},
   "source": [
    "## 2. Solution Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619d0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     product_description  \\\n",
      "0      This is a premium quality product   \n",
      "1                 Budget friendly option   \n",
      "2  Premium design with advanced features   \n",
      "3                         Standard model   \n",
      "\n",
      "                                        tokens  \n",
      "0     [This, is, a, premium, quality, product]  \n",
      "1                   [Budget, friendly, option]  \n",
      "2  [Premium, design, with, advanced, features]  \n",
      "3                            [Standard, model]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    \"product_description\": [\n",
    "        \"This is a premium quality product\",\n",
    "        \"Budget friendly option\",\n",
    "        \"Premium design with advanced features\",\n",
    "        \"Standard model\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tokenizing text into words\n",
    "df['tokens']=df['product_description'].str.split()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abd312",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Explanation (What is happening)\n",
    "\n",
    "- **str.split()**  \n",
    "  → Splits the text into words based on whitespace (by default)\n",
    "\n",
    "For example:\n",
    "- `\"This is a premium quality product\"` → `[\"This\", \"is\", \"a\", \"premium\", \"quality\", \"product\"]`\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary / Takeaways\n",
    "\n",
    "By solving this problem, I learned:\n",
    "\n",
    "1. How to split text into individual tokens (words)\n",
    "2. The importance of tokenization for NLP tasks\n",
    "3. How tokenized words can be used as features in models\n",
    "4. Why tokenization is the first step in most NLP pipelines\n",
    "\n",
    "This is a key **NLP preprocessing step** and shows your understanding of how to handle text data for machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "Next, I’ll move toward:\n",
    "- Tokenizing text into sentences\n",
    "- Removing stop words and punctuation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
